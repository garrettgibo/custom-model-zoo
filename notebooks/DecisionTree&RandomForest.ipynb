{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6G8uG6sm-gGj"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VgqMGh9-gGm"
   },
   "source": [
    "https://en.wikipedia.org/wiki/Decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xgiZBDKB-gGn"
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9G43Tqa-gGn"
   },
   "outputs": [],
   "source": [
    "from modelzoo.models import Model\n",
    "class DecisionTree(Model):\n",
    "    \"\"\"\n",
    "    Decision Tree Classifier\n",
    "\n",
    "    Attributes:\n",
    "        root: Root Node of the tree.\n",
    "        max_depth: Max depth allowed for the tree\n",
    "        size_allowed : Min_size split, smallest size allowed for split\n",
    "        n_features: Number of features to use during building the tree.\n",
    "            (Random Forest)\n",
    "        n_split:  Number of split for each feature. (Random Forest)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=1000, size_allowed=1, n_features=None, n_split=None):\n",
    "        \"\"\"Decision Tree initialization.\n",
    "\n",
    "        :param max_depth: maximum amount of recursion for tree to create nodes\n",
    "        :param size_allowed: minimum node size\n",
    "        :param n_features: number of features to evaluate for decisions\n",
    "        :param n_split: number of splits to do on features\n",
    "        \"\"\"\n",
    "        self.root = 1\n",
    "        self.max_depth = max_depth\n",
    "        self.size_allowed = size_allowed\n",
    "        self.n_features = n_features\n",
    "        self.n_split = n_split\n",
    "\n",
    "    class Node:\n",
    "        \"\"\"\n",
    "        Node Class for the building the tree.\n",
    "\n",
    "        Attribute:\n",
    "            threshold: The threshold like if x1 < threshold, for\n",
    "                spliting.\n",
    "            feature: The index of feature on this current node.\n",
    "            left: Pointer to the node on the left.\n",
    "            right: Pointer to the node on the right.\n",
    "            pure: Bool, describe if this node is pure.\n",
    "            predict: Class, indicate what the most common Y on this\n",
    "                node.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, threshold: float = None, feature: int = None):\n",
    "            \"\"\"Decision Tree Node initialization.\n",
    "\n",
    "            :param threshold: threshold to split features on\n",
    "            :param feature: feature index to split on\n",
    "            \"\"\"\n",
    "            self.threshold = threshold\n",
    "            self.feature = feature\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.pure = False\n",
    "            self.depth = 1\n",
    "            self.predict = -1\n",
    "\n",
    "    def entropy(self, labels: np.array) -> float:\n",
    "        \"\"\"Calculate entropy for provided labels.\n",
    "\n",
    "        :param labels: vector of labels to calculate entropy on\n",
    "        :returns: calculated entropy\n",
    "        \"\"\"\n",
    "        entro = 0\n",
    "        classes, counts = np.unique(labels, return_counts=True)\n",
    "        counts = counts / sum(counts)  # normalize counts to get prob of class\n",
    "        for count in counts:\n",
    "            if count == 0:\n",
    "                continue\n",
    "            entro -= count * np.log(count)\n",
    "        return entro\n",
    "\n",
    "    def information_gain(\n",
    "        self, values: np.array, labels: np.array, threshold: float\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate the information gain, by using entropy function.\n",
    "\n",
    "        IG(Z) = H(X) - H(X|Z)\n",
    "\n",
    "        :param values: single vector of values to calculate IG\n",
    "        :param labels: vector of all labels\n",
    "        :param threshold: threshold to calculate IG off of\n",
    "        :returns: calculate IG based off information gain formula\n",
    "        \"\"\"\n",
    "        left_side = values < threshold\n",
    "        left_prop = len(values[left_side]) / len(values)\n",
    "        right_prop = 1 - left_prop\n",
    "\n",
    "        left_entropy = self.entropy(labels[left_side])\n",
    "        right_entropy = self.entropy(labels[~left_side])\n",
    "\n",
    "        return self.entropy(labels) - (\n",
    "            left_prop * left_entropy + right_prop * right_entropy\n",
    "        )\n",
    "\n",
    "    def find_rules(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Helper method to find the split rules.\n",
    "\n",
    "        Splitting rules are found by finding all unique values in a feature,\n",
    "        then finding all the midpoints for the unique values.\n",
    "\n",
    "        :param data: matrix or 2-D numpy array, represnting training instances\n",
    "        :returns: 2-D array of all possible splits for features\n",
    "        \"\"\"\n",
    "        rules = []\n",
    "        # transpose data to get features(columns)\n",
    "        for feature in data.T:\n",
    "            unique_values = np.unique(feature)\n",
    "            mids = np.mean([unique_values[:-1], unique_values[1:]], axis=0)\n",
    "            rules.append(mids)\n",
    "        return rules\n",
    "\n",
    "    def next_split(self, data: np.ndarray, labels: np.array) -> Tuple[float, int]:\n",
    "        \"\"\"Helper method to find the split with most information.\n",
    "\n",
    "        :param data: matrix or 2-D numpy array, represnting training instances\n",
    "            Each training instance is a feature vector.\n",
    "        :param labels: label contains the corresponding labels. There might be\n",
    "            multiple (i.e., > 2) classes.\n",
    "        \"\"\"\n",
    "        rules = self.find_rules(data)\n",
    "        max_info = -1\n",
    "        num_col = 1\n",
    "        threshold = 1\n",
    "\n",
    "        # when number of features wasn't set, use all features\n",
    "        if self.n_features is None:\n",
    "            index_col = np.arange(data.shape[1])\n",
    "        else:\n",
    "            if isinstance(self.n_features, int):\n",
    "                num_index = self.n_features\n",
    "            # if num of featuers is 'sqrt' use sqrt of total number of features\n",
    "            elif isinstance(self.n_features, str):\n",
    "                num_index = round(np.sqrt(data.shape[1]))\n",
    "                np.random.seed()\n",
    "                index_col = np.random.choice(data.shape[1], num_index, replace=False)\n",
    "\n",
    "        # Moving through columns\n",
    "        for i in index_col:\n",
    "            count_temp_rules = len(rules[i])\n",
    "\n",
    "            # when number of splits wasn't set, use all splits\n",
    "            if self.n_split is None:\n",
    "                index_rules = np.arange(count_temp_rules)\n",
    "            else:\n",
    "                if isinstance(self.n_split, int):\n",
    "                    num_rules = self.n_split\n",
    "                elif isinstance(self.n_split, str):\n",
    "                    num_rules = round(np.sqrt(data.shape[0]))\n",
    "                    if num_rules > count_temp_rules:\n",
    "                        num_rules = count_temp_rules\n",
    "                    np.random.seed()\n",
    "                    index_rules = np.random.choice(\n",
    "                        count_temp_rules, num_rules, replace=False\n",
    "                    )\n",
    "\n",
    "            # find split and threshold that results in maximum information gain\n",
    "            for j in index_rules:\n",
    "                info = self.information_gain(data.T[i], labels, rules[i][j])\n",
    "                if info > max_info:\n",
    "                    max_info = info\n",
    "                    num_col = i\n",
    "                    threshold = rules[i][j]\n",
    "        return threshold, num_col\n",
    "\n",
    "    def build_tree(self, X: np.ndarray, y: np.array, depth: int) -> Node:\n",
    "        \"\"\" Helper function for building the tree.\n",
    "\n",
    "        :param X: full data set to train from\n",
    "        :param y: full vector of labels\n",
    "        :returns: root Node\n",
    "        \"\"\"\n",
    "        first_threshold, first_feature = self.next_split(X, y)\n",
    "        current = self.Node(first_threshold, first_feature)\n",
    "\n",
    "        # base case 1 to end build early\n",
    "        if (\n",
    "            depth > self.max_depth\n",
    "            or first_feature is None\n",
    "            or X.shape[0] == self.size_allowed\n",
    "        ):\n",
    "            current.predict = np.argmax(np.bincount(y))\n",
    "            current.pure = True\n",
    "            return current\n",
    "\n",
    "        # base case 2: node has become a leaf\n",
    "        if len(np.unique(y)) == 1:\n",
    "            current.pure = True\n",
    "            current.predict = y[0]\n",
    "            return current\n",
    "\n",
    "        # Find the left node index with feature i <= threshold\n",
    "        # Right with feature i > threshold.\n",
    "        left_index = X.T[first_feature] <= first_threshold\n",
    "        right_index = X.T[first_feature] > first_threshold\n",
    "\n",
    "        # base case 3: either side is empty\n",
    "        if sum(left_index) == 0 or sum(right_index) == 0:\n",
    "            # NOTE this is being set to the first label, but it may be better\n",
    "            # to set this to the most common label\n",
    "            current.predict = y[0]\n",
    "            current.pure = True\n",
    "            return current\n",
    "\n",
    "        # recusively build rest of tree\n",
    "        left_X, left_y = X[left_index, :], y[left_index]\n",
    "        current.left = self.build_tree(left_X, left_y, depth + 1)\n",
    "\n",
    "        right_X, right_y = X[right_index, :], y[right_index]\n",
    "        current.right = self.build_tree(right_X, right_y, depth + 1)\n",
    "\n",
    "        return current\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.array):\n",
    "        \"\"\" Fits the Decision Tree model based on the training data.\n",
    "\n",
    "        :param X: matrix or 2-D numpy array, represnting training instances.\n",
    "            Each training instance is a feature vector.\n",
    "        :param y: labels for data. There might be multiple (i.e., > 2) classes.\n",
    "        \"\"\"\n",
    "        self.root = self.build_tree(X, y, 1)\n",
    "        return self\n",
    "\n",
    "    def ind_predict(self, vector: np.array) -> int:\n",
    "        \"\"\"Predict the most likely class label of one test instance.\n",
    "\n",
    "        :param vector: single vector to predict\n",
    "        :returns: class predicted\n",
    "        \"\"\"\n",
    "        current = self.root\n",
    "        while not current.pure:\n",
    "            feature = current.feature\n",
    "            threshold = current.threshold\n",
    "            if vector[feature] < threshold:\n",
    "                current = current.left\n",
    "            else:\n",
    "                current = current.right\n",
    "        return current.predict\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.array:\n",
    "        \"\"\"Predict labels for entire dataset.\n",
    "\n",
    "        :param X: matrix or 2-D numpy array, represnting training instances.\n",
    "            Each training instance is a feature vector.\n",
    "        :param y: labels for data. There might be multiple (i.e., > 2) classes.\n",
    "        :returns: predictions of all instances in a list.\n",
    "        \"\"\"\n",
    "        return np.array([self.ind_predict(vect) for vect in X])\n",
    "\n",
    "    def score(self, data: np.ndarray, labels: np.array, datatype=\"Test\") -> float:\n",
    "        \"\"\"Wrapper around predict to also get accuracy.\n",
    "\n",
    "        :param data: matrix or 2-D numpy array, represnting training instances.\n",
    "            Each training instance is a feature vector.\n",
    "        :param labels: labels for data. There might be multiple (i.e., > 2) classes.\n",
    "        :returns: avg_accuracy of predictions\n",
    "        \"\"\"\n",
    "        pred = self.predict(data)\n",
    "        avg_accuracy = (pred == labels).mean()\n",
    "        print(f\"{datatype} accuracy: {avg_accuracy}\")\n",
    "        return avg_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2uPtiFV-gGr"
   },
   "outputs": [],
   "source": [
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umUfc-Kj-gGv"
   },
   "outputs": [],
   "source": [
    "X = np.array(wine)[:, :-1]\n",
    "y = np.array(wine)[:, -1]\n",
    "y = np.array(y.flatten()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8lOQ1vd-gGx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5p2aQgpE-gGz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMzJB5Ot-gG2",
    "outputId": "c939a2b6-1f8d-4e5b-f013-e3afb125ad9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTree"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTree()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ul2dkqPh-gG5"
   },
   "source": [
    "### Train Error should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrinqIMZ-gG6",
    "outputId": "5d0a4110-6d35-4700-b6dc-e4f5bf928f65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(X_train)\n",
    "1 - (pred == y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMu0yRH1-gG8"
   },
   "source": [
    "### Test Accuracy should be around 0.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mw0_bBUU-gG8"
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIVcRBSC-gG-",
    "outputId": "18a7d8fb-17c9-49f1-a0a2-a0cdc9aa1c01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.628125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuWhhXnn-gG_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.628125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.628125"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSgiIwTc-gHB"
   },
   "source": [
    "https://en.wikipedia.org/wiki/Random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0gYN1_H-gHB"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # adds progress bar fro fitting of forest\n",
    "class RandomForest(Model):\n",
    "    \"\"\"RandomForest Classifier\n",
    "\n",
    "    Attributes:\n",
    "        n_trees: Number of trees.\n",
    "        trees: List store each individule tree\n",
    "        n_features: Number of features to use during building each individule tree.\n",
    "        n_split: Number of split for each feature.\n",
    "        max_depth: Max depth allowed for the tree\n",
    "        size_allowed : Min_size split, smallest size allowed for split\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_trees=25,\n",
    "        n_features=\"sqrt\",\n",
    "        n_split=None,\n",
    "        max_depth=1000,\n",
    "        size_allowed=1,\n",
    "    ):\n",
    "        \"\"\"Random Forest initialization\"\"\"\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.n_features = n_features\n",
    "        self.n_split = n_split\n",
    "        self.max_depth = max_depth\n",
    "        self.size_allowed = size_allowed\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.array) -> None:\n",
    "        \"\"\" Fits the Random Forest model based on the training data.\n",
    "\n",
    "        :param X: matrix or 2-D numpy array, represnting training instances.\n",
    "            Each training instance is a feature vector.\n",
    "        :param y: labels for data. There might be multiple (i.e., > 2) classes.\n",
    "        \"\"\"\n",
    "        for i in tqdm(range(self.n_trees), desc=\"Fitting Forest\"):\n",
    "            np.random.seed()\n",
    "            # initialize tree with all parameters from forest\n",
    "            temp_clf = DecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                size_allowed=self.size_allowed,\n",
    "                n_features=self.n_features,\n",
    "                n_split=self.n_split,\n",
    "            )\n",
    "            temp_clf.fit(X, y)\n",
    "            self.trees.append(temp_clf)\n",
    "        return self\n",
    "\n",
    "    def ind_predict(self, vector: np.array) -> float:\n",
    "        \"\"\"Predict the most likely class label of one test instance.\n",
    "\n",
    "        :param vector: single vector to predict\n",
    "        :returns: class predicted\n",
    "        \"\"\"\n",
    "        # predict using majority rule from doing predictions from all trees\n",
    "        results = np.array([tree.ind_predict(vector) for tree in self.trees])\n",
    "        labels, counts = np.unique(results, return_counts=True)\n",
    "        return labels[np.argmax(counts)]\n",
    "\n",
    "    def predict_all(self, X: np.ndarray) -> np.array:\n",
    "        \"\"\"Predict labels for entire dataset.\n",
    "\n",
    "        :param X: matrix or 2-D numpy array, represnting training instances.\n",
    "            Each training instance is a feature vector.\n",
    "        :param y: labels for data. There might be multiple (i.e., > 2) classes.\n",
    "        :returns: predictions of all instances in a list.\n",
    "        \"\"\"\n",
    "        return np.array([self.ind_predict(vect) for vect in X])\n",
    "\n",
    "    def score(self, data: np.ndarray, labels: np.array, datatype=\"Test\"):\n",
    "        \"\"\"Wrapper around predict_all to also get accuracy.\n",
    "\n",
    "        :param data: matrix or 2-D numpy array, represnting training instances.\n",
    "            Each training instance is a feature vector.\n",
    "        :param labels: labels for data. There might be multiple (i.e., > 2) classes.\n",
    "        :returns: avg_accuracy of predictions\n",
    "        \"\"\"\n",
    "        pred = self.predict_all(data)\n",
    "        avg_accuracy = (pred == labels).mean()\n",
    "        print(f\"{datatype} accuracy: {avg_accuracy}\")\n",
    "        return avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZqOzdjh-gHG"
   },
   "source": [
    "### Test Accruacy should be greater than 0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRTbwtIG-gHH",
    "outputId": "b20072b8-597e-477b-f5fb-33c4f01b993c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Forest: 100%|██████████| 100/100 [03:38<00:00,  2.19s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForest"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForest(n_trees= 100, n_split=None)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uoq3tFQ_-gHJ",
    "outputId": "9f200c99-d3fa-47bd-9541-eb98f34189e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7375"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict_all(X_test)\n",
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUwAMpuT-gHK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7375"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DecisionTree&RandomForest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
