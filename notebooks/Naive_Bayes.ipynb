{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"New_Naive_Bayes.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"j4zcwLIJuG1O","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KDABJK5kuG1R","colab_type":"text"},"source":["# Naive Bayes Classifier \n","It is a conditional probability model, with formula: <br>\n","$ P(C| x_1, x_2, x_3, ...) = \\frac{P(C)P(X|C)}{P(X)}$ <br>\n","It is naive because we have naive assumption such that every pair of features are independent from each other given C.<br>\n","So we can rewrite the formula as: <br>\n","$ P(C| x_1, x_2, x_3, ...) = P(C)P(x_1|C)P(x_2|C)... = P(C)\\prod^{n}_{i=1} P(x_i|C)$"]},{"cell_type":"code","metadata":{"id":"_JUB-OvnuG1S","colab_type":"code","colab":{}},"source":["class Naive_Bayes():\n","    \"\"\"\n","    \n","    Naive Bayes classifer\n","    \n","    Attributes:\n","        prior: P(Y)\n","        likelihood: P(X_j | Y)\n","    \"\"\"\n","    \n","    def __init__(self):\n","        \"\"\"\n","            Some initializations, if neccesary\n","        \"\"\"\n","        \n","        self.model_name = 'Naive Bayes'\n","    \n","    \n","    def fit(self, X_train, y_train):\n","        \n","        \"\"\" \n","            The fit function fits the Naive Bayes model based on the training data. \n","            Here, we assume that all the features are **discrete** features. \n","            \n","            X_train is a matrix or 2-D numpy array, represnting training instances. \n","            Each training instance is a feature vector. \n","\n","            y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n","        \"\"\"\n","        \n","        \"\"\"\n","            TODO: 1. Modify and add some codes to the following for-loop\n","                     to compute the correct prior distribution of all y labels.\n","                  2. Make sure they are normalized to a distribution.\n","        \"\"\"\n","        self.prior = dict()\n","        for y in y_train:\n","            self.prior[f'Y = {y}'] = 1\n","            \n","        \"\"\"\n","            TODO: 3. Modify and add some codes to the following for-loops\n","                     to compute the correct likelihood P(X_j | Y).\n","                  4. Make sure they are normalized to distributions.\n","        \"\"\"\n","        self.likelihood = dict()\n","        for x, y in zip(X_train, y_train):\n","            for j in range(len(x)):\n","                self.likelihood[f'X{j} = {x[j]} | Y = {y}'] = 1\n","\n","        \"\"\"\n","            TODO: 5. Think about whether we really need P(X_1 = x_1, X_2 = x_2, ..., X_d = x_d)\n","                     in practice?\n","                  6. Does this really matter for the final classification results?\n","        \"\"\"\n","\n","        \n","    def ind_predict(self, x : list):\n","        \n","        \"\"\" \n","            Predict the most likely class label of one test instance based on its feature vector x.\n","        \"\"\"\n","        \n","        \"\"\"\n","            TODO: 7. Enumerate all possible class labels and compute the likelihood \n","                     based on the given feature vector x. Don't forget to incorporate \n","                     both the prior and likelihood.\n","                  8. Pick the label with the higest probability. \n","                  9. How to deal with very small probability values, especially\n","                     when the feature vector is of a high dimension. (Hint: log)\n","                  10. How to how to deal with unknown feature values?\n","        \"\"\"\n","        \n","        ret, max_prob = None, 0\n","        for y in []:\n","            prob = 1\n","            for j in range(len(x)):\n","                prob *= 1\n","            if prob > max_prob:\n","                max_prob = prob\n","                ret = y\n","        return ret\n","    \n","    \n","    def predict(self, X):\n","        \n","        \"\"\"\n","            X is a matrix or 2-D numpy array, represnting testing instances. \n","            Each testing instance is a feature vector. \n","            \n","            Return the predictions of all instances in a list.\n","        \"\"\"\n","        \n","        \"\"\"\n","            TODO: 11. Revise the following for-loop to call ind_predict to get predictions.\n","        \"\"\"\n","        \n","        ret = []\n","        for x in X:\n","            ret.append('L')\n","        \n","        return ret"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0g4b0OM7uG1U","colab_type":"code","colab":{}},"source":["url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data'\n","col = ['class_name','left_weight','left_distance','right_weight','right_distance']\n","data = pd.read_csv(url, delimiter = ',', names = col)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hq7_a9C0uG1W","colab_type":"code","outputId":"9f21f418-fa7d-4a77-dfc0-186ec3424e61","colab":{}},"source":["data"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class_name</th>\n","      <th>left_weight</th>\n","      <th>left_distance</th>\n","      <th>right_weight</th>\n","      <th>right_distance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>B</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>R</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>R</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>R</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>R</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>L</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <td>621</td>\n","      <td>L</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>622</td>\n","      <td>L</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <td>623</td>\n","      <td>L</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <td>624</td>\n","      <td>B</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>625 rows Ã— 5 columns</p>\n","</div>"],"text/plain":["    class_name  left_weight  left_distance  right_weight  right_distance\n","0            B            1              1             1               1\n","1            R            1              1             1               2\n","2            R            1              1             1               3\n","3            R            1              1             1               4\n","4            R            1              1             1               5\n","..         ...          ...            ...           ...             ...\n","620          L            5              5             5               1\n","621          L            5              5             5               2\n","622          L            5              5             5               3\n","623          L            5              5             5               4\n","624          B            5              5             5               5\n","\n","[625 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"ZAsYzCPzuG1Z","colab_type":"code","outputId":"60ec9f98-d513-479d-a6cf-e65d50d2d3c0","colab":{}},"source":["data.class_name.value_counts()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["R    288\n","L    288\n","B     49\n","Name: class_name, dtype: int64"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"0JjkLsxGuG1e","colab_type":"code","colab":{}},"source":["X = np.matrix(data.iloc[:,1:])\n","y = data.class_name\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,random_state = 88)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZxU7v9SxuG1f","colab_type":"code","colab":{}},"source":["clf = Naive_Bayes()\n","clf.fit(X_train, y_train)\n","y_test = np.array(y_test)\n","y_hat = clf.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y2RZ2jYsuG1h","colab_type":"text"},"source":["Overall Accuracy"]},{"cell_type":"code","metadata":{"id":"MpfumdTCuG1i","colab_type":"code","outputId":"6925cbf8-873b-49f0-80b2-754f569c3f8b","colab":{}},"source":["sum(y_hat == y_test)/ 207  # you should get something like 0.88"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.41545893719806765"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"itazR0opuG1k","colab_type":"code","outputId":"9efcc83c-1b5a-4a92-9373-07be8301d61b","colab":{}},"source":["len(y_test)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["207"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"HAUQN8uFuG1l","colab_type":"code","outputId":"5039b87d-c601-4a6f-9310-caf7e9283c5c","colab":{}},"source":["len(y_hat)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["207"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"ZfZ9Mg4wuG1n","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}